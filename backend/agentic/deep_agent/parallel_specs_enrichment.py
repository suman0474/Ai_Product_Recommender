# agentic/deep_agent/parallel_specs_enrichment.py
# =============================================================================
# PARALLEL 3-SOURCE SPECIFICATION ENRICHMENT
# =============================================================================
#
# Runs 3 specification sources in PARALLEL:
# 1. USER_SPECIFIED - Extract explicit specs from user input (MANDATORY)
# 2. LLM_GENERATED - Generate specs for product type
# 3. STANDARDS_BASED - Extract from standards documents
#
# All results stored in Deep Agent Memory, then deduplicated and merged.
#
# =============================================================================

import logging
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Any, List, Optional
from datetime import datetime
from uuid import uuid4

# Local imports (within deep_agent package)
from .memory import (
    DeepAgentMemory,
    ParallelEnrichmentResult,
    SpecificationSource
)
from .llm_specs_generator import extract_user_specified_specs, generate_llm_specs
from .standards_deep_agent import run_standards_deep_agent_batch
from .spec_output_normalizer import normalize_specification_output, normalize_key
logger = logging.getLogger(__name__)


# =============================================================================
# DEDUPLICATION AND MERGE LOGIC
# =============================================================================

def deduplicate_and_merge_specifications(
    user_specs: Dict[str, Any],
    llm_specs: Dict[str, Any],
    standards_specs: Dict[str, Any]
) -> Dict[str, SpecificationSource]:
    """
    Merge specifications from 3 sources with deduplication.

    Priority for CONFLICTING keys:
    1. user_specified (ALWAYS wins - mandatory, never overwritten)
    2. standards_specs (authoritative technical source)
    3. llm_specs (fallback for missing keys)

    For NON-CONFLICTING keys: include ALL.

    Args:
        user_specs: Specifications explicitly stated by user (MANDATORY)
        llm_specs: Specifications generated by LLM
        standards_specs: Specifications from standards documents

    Returns:
        Dict[str, SpecificationSource] with merged specs and metadata
    """
    merged: Dict[str, SpecificationSource] = {}
    timestamp = datetime.now().isoformat()

    # First: Add ALL user-specified specs (mandatory, never overwritten)
    for key, value in user_specs.items():
        if value and str(value).lower() not in ["null", "none", ""]:
            merged[key] = SpecificationSource(
                value=value,
                source="user_specified",
                confidence=1.0,  # User specs have highest confidence
                standard_reference=None,
                timestamp=timestamp
            )
            logger.debug(f"[MERGE] Added user-specified: {key} = {value}")

    # Second: Add standards specs (only if key not in user_specs)
    for key, value_data in standards_specs.items():
        if key in merged:
            logger.debug(f"[MERGE] Skipping standards '{key}' - user spec exists")
            continue

        # Handle both simple values and dict structures
        if isinstance(value_data, dict):
            value = value_data.get("value", str(value_data))
            confidence = value_data.get("confidence", 0.9)
            std_ref = value_data.get("standard_reference", None)
        else:
            value = value_data
            confidence = 0.9
            std_ref = None

        if value and str(value).lower() not in ["null", "none", "", "extracted value or null"]:
            merged[key] = SpecificationSource(
                value=value,
                source="standards",
                confidence=confidence,
                standard_reference=std_ref,
                timestamp=timestamp
            )
            logger.debug(f"[MERGE] Added standards: {key} = {value}")

    # Third: Add LLM specs (only if key not in user_specs or standards)
    for key, value_data in llm_specs.items():
        if key in merged:
            logger.debug(f"[MERGE] Skipping LLM '{key}' - existing spec")
            continue

        # Handle both simple values and dict structures
        if isinstance(value_data, dict):
            value = value_data.get("value", str(value_data))
            confidence = value_data.get("confidence", 0.7)
        else:
            value = value_data
            confidence = 0.7

        if value and str(value).lower() not in ["null", "none", ""]:
            merged[key] = SpecificationSource(
                value=value,
                source="llm_generated",
                confidence=confidence,
                standard_reference=None,
                timestamp=timestamp
            )
            logger.debug(f"[MERGE] Added LLM: {key} = {value}")

    logger.info(f"[MERGE] Merged {len(merged)} specs from 3 sources")

    # Log source breakdown
    user_count = sum(1 for s in merged.values() if s.get("source") == "user_specified")
    std_count = sum(1 for s in merged.values() if s.get("source") == "standards")
    llm_count = sum(1 for s in merged.values() if s.get("source") == "llm_generated")
    logger.info(f"[MERGE] Breakdown: {user_count} user, {std_count} standards, {llm_count} LLM")

    return merged


# =============================================================================
# PARALLEL ENRICHMENT FUNCTION
# =============================================================================

def run_parallel_3_source_enrichment(
    items: List[Dict[str, Any]],
    user_input: str,
    session_id: Optional[str] = None,
    domain_context: Optional[str] = None,
    safety_requirements: Optional[Dict[str, Any]] = None,
    memory: Optional[DeepAgentMemory] = None
) -> Dict[str, Any]:
    """
    Run parallel 3-source specification enrichment for all items.

    Executes 3 parallel threads:
    1. USER_SPECIFIED: Extract explicit user requirements (MANDATORY)
    2. LLM_GENERATED: Generate specs for each product type
    3. STANDARDS_BASED: Extract from standards documents

    All results are stored in Deep Agent Memory, deduplicated, and merged.

    Args:
        items: List of identified items with 'name', 'category', 'sample_input'
        user_input: Original user input text
        session_id: Session ID for logging
        domain_context: Domain context (e.g., "Oil & Gas")
        safety_requirements: Safety context (e.g., {"sil_level": "SIL2"})
        memory: Optional DeepAgentMemory instance (creates new if None)

    Returns:
        Dict with enriched items and metadata
    """
    start_time = time.time()
    session_id = session_id or f"parallel-{uuid4().hex[:8]}"

    logger.info(f"[PARALLEL_3SOURCE] Starting enrichment for {len(items)} items")
    logger.info(f"[PARALLEL_3SOURCE] Session: {session_id}")

    if not items:
        return {
            "success": True,
            "items": [],
            "metadata": {
                "total_items": 0,
                "processing_time_ms": 0
            }
        }

    # Create or use existing memory
    if memory is None:
        memory = DeepAgentMemory()

    # Results containers
    user_specs_results = {}
    llm_specs_results = {}
    standards_specs_results = {}

    # =========================================================================
    # PARALLEL EXECUTION: 3 SOURCES
    # =========================================================================

    def run_user_specs_extraction():
        """Thread 1: Extract user-specified specs"""
        logger.info("[THREAD-USER] Starting user specs extraction...")
        try:
            for item in items:
                item_name = item.get("name") or item.get("product_name", "Unknown")
                result = extract_user_specified_specs(
                    user_input=user_input,
                    product_type=item_name,
                    sample_input=item.get("sample_input", "")
                )
                user_specs_results[item_name] = result.get("specifications", {})
            logger.info(f"[THREAD-USER] Completed for {len(items)} items")
            return True
        except Exception as e:
            logger.error(f"[THREAD-USER] Failed: {e}")
            return False

    def run_llm_specs_generation():
        """Thread 2: Generate LLM specs"""
        logger.info("[THREAD-LLM] Starting LLM specs generation...")
        try:
            for item in items:
                item_name = item.get("name") or item.get("product_name", "Unknown")
                category = item.get("category", "Industrial Instrument")
                result = generate_llm_specs(
                    product_type=item_name,
                    category=category,
                    context=item.get("sample_input", "")
                )
                raw_specs = result.get("specifications", {})
                
                # NORMALIZE: Flatten nested sections and extract clean values
                # This converts {"Communication": {"protocol": {"value": "HART", "confidence": 0.6}}}
                # Into: {"protocol": "HART"}
                normalized_specs = normalize_specification_output(raw_specs, preserve_ghost_values=False)
                
                # Remove internal keys (like _ghost)
                clean_specs = {k: v for k, v in normalized_specs.items() if not k.startswith('_')}
                
                llm_specs_results[item_name] = clean_specs
                logger.debug(f"[THREAD-LLM] Normalized {len(raw_specs)} -> {len(clean_specs)} specs for {item_name}")
            logger.info(f"[THREAD-LLM] Completed for {len(items)} items")
            return True
        except Exception as e:
            logger.error(f"[THREAD-LLM] Failed: {e}")
            return False


    def run_standards_extraction():
        """Thread 3: Extract standards-based specs"""
        logger.info("[THREAD-STANDARDS] Starting standards extraction...")
        try:
            result = run_standards_deep_agent_batch(
                items=items,
                session_id=session_id,
                domain_context=domain_context,
                safety_requirements=safety_requirements
            )

            if result.get("success"):
                enriched_items = result.get("items", [])
                for item in enriched_items:
                    item_name = item.get("name") or item.get("product_name", "Unknown")
                    raw_specs = item.get("standards_specifications", {})
                    
                    # NORMALIZE: Flatten nested structures and clean values
                    normalized_specs = normalize_specification_output(raw_specs, preserve_ghost_values=False)
                    clean_specs = {k: v for k, v in normalized_specs.items() if not k.startswith('_')}
                    
                    standards_specs_results[item_name] = clean_specs

            logger.info(f"[THREAD-STANDARDS] Completed for {len(items)} items")
            return True
        except Exception as e:
            logger.error(f"[THREAD-STANDARDS] Failed: {e}")
            return False


    # Execute all 3 threads in parallel
    logger.info("[PARALLEL_3SOURCE] Launching 3 parallel threads...")

    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = {
            executor.submit(run_user_specs_extraction): "user_specs",
            executor.submit(run_llm_specs_generation): "llm_specs",
            executor.submit(run_standards_extraction): "standards_specs"
        }

        results_status = {}
        for future in as_completed(futures):
            thread_name = futures[future]
            try:
                success = future.result()
                results_status[thread_name] = success
                logger.info(f"[PARALLEL_3SOURCE] Thread '{thread_name}' completed: {success}")
            except Exception as e:
                results_status[thread_name] = False
                logger.error(f"[PARALLEL_3SOURCE] Thread '{thread_name}' exception: {e}")

    # =========================================================================
    # MERGE AND DEDUPLICATE
    # =========================================================================

    logger.info("[PARALLEL_3SOURCE] Merging results from all sources...")
    enriched_items = []

    for item in items:
        item_name = item.get("name") or item.get("product_name", "Unknown")
        item_id = f"{session_id}_{item.get('number', 0)}_{item_name}"

        # Get results from each source
        user_specs = user_specs_results.get(item_name, {})
        llm_specs = llm_specs_results.get(item_name, {})
        standards_specs = standards_specs_results.get(item_name, {})

        # Deduplicate and merge
        merged_specs = deduplicate_and_merge_specifications(
            user_specs=user_specs,
            llm_specs=llm_specs,
            standards_specs=standards_specs
        )

        # Create ParallelEnrichmentResult
        enrichment_result: ParallelEnrichmentResult = {
            "item_id": item_id,
            "item_name": item_name,
            "item_type": item.get("type", "instrument"),
            "user_specified_specs": user_specs,
            "llm_generated_specs": llm_specs,
            "standards_specs": standards_specs,
            "merged_specs": merged_specs,
            "enrichment_metadata": {
                "user_specs_count": len(user_specs),
                "llm_specs_count": len(llm_specs),
                "standards_specs_count": len(standards_specs),
                "merged_specs_count": len(merged_specs),
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }
        }

        # Store in memory
        memory.store_parallel_enrichment_result(item_id, enrichment_result)

        # Create enriched item for return
        enriched_item = item.copy()

        # Add all spec sources
        enriched_item["user_specified_specs"] = user_specs
        enriched_item["llm_generated_specs"] = llm_specs
        enriched_item["standards_specifications"] = standards_specs

        # Add merged specs (flattened for display)
        enriched_item["combined_specifications"] = {
            k: v for k, v in merged_specs.items()
        }

        # Add flattened specifications for backward compatibility
        enriched_item["specifications"] = {
            k: v.get("value", v) if isinstance(v, dict) else v
            for k, v in merged_specs.items()
        }

        # Add metadata
        enriched_item["standards_info"] = {
            "enrichment_status": "success",
            "sources_used": ["user_specified", "llm_generated", "standards"],
            "user_specs_count": len(user_specs),
            "llm_specs_count": len(llm_specs),
            "standards_specs_count": len(standards_specs)
        }

        enriched_items.append(enriched_item)

    processing_time = int((time.time() - start_time) * 1000)

    logger.info(f"[PARALLEL_3SOURCE] Enrichment complete in {processing_time}ms")
    logger.info(f"[PARALLEL_3SOURCE] Enriched {len(enriched_items)} items")

    return {
        "success": True,
        "items": enriched_items,
        "metadata": {
            "total_items": len(enriched_items),
            "processing_time_ms": processing_time,
            "session_id": session_id,
            "thread_results": results_status,
            "memory_stats": memory.get_memory_stats()
        },
        "memory": memory
    }


# =============================================================================
# EXPORTS
# =============================================================================

__all__ = [
    "run_parallel_3_source_enrichment",
    "deduplicate_and_merge_specifications"
]
