# agentic/schema_workflow.py
# Complete Schema Creation & Generation Workflow
#
# Orchestrates entire schema lifecycle:
# 1. Check database for existing schema
# 2. If found: Load and return (instant!)
# 3. If not found: Generate via PPI workflow
# 4. Apply Phase 1 optimizations (deduplication, fast-fail)
# 5. Apply Phase 2 optimizations (parallel processing)
# 6. Apply Phase 3 optimizations (async concurrency)
# 7. Store result back to database
# 8. Return to user
#
# Expected Performance:
# - Cached schema: < 1 second
# - New single product: 100-120 seconds (with all optimizations)
# - Multiple products: 100-120 seconds (async parallel)
# - 4.5x speedup vs baseline (437s â†’ 100-120s)

import logging
import asyncio
import time
from typing import Dict, List, Any, Optional, Tuple

logger = logging.getLogger(__name__)


class SchemaWorkflow:
    """
    Complete schema creation and generation workflow.

    Handles:
    1. Database lookup (checks if schema exists)
    2. PPI generation (if not cached)
    3. Phase 1 optimizations (caching, fast-fail)
    4. Phase 2 optimizations (parallel ThreadPoolExecutor)
    5. Phase 3 optimizations (async/await concurrency)
    6. Storage back to database
    7. Return to user

    Usage:
        >>> workflow = SchemaWorkflow()
        >>> schema = await workflow.get_or_generate_schema("Temperature Transmitter")
        >>> # Automatic: checks cache, generates if needed, stores result
    """

    def __init__(self, use_phase3_async: bool = True):
        """
        Initialize schema workflow.

        Args:
            use_phase3_async: Use async/await for concurrency (default True)
                             If False, falls back to Phase 2 ThreadPoolExecutor
        """
        self.use_phase3_async = use_phase3_async
        self.session_cache = {}  # Session-level cache (FIX #A1 from Phase 1)

        logger.info("[SCHEMA_WORKFLOW] Initialized")
        logger.info(f"[SCHEMA_WORKFLOW] Phase 3 async: {'enabled' if use_phase3_async else 'disabled'}")

    async def get_or_generate_schema(
        self,
        product_type: str,
        session_id: Optional[str] = None,
        force_regenerate: bool = False
    ) -> Dict[str, Any]:
        """
        Get schema for product, generating if necessary.

        Complete workflow:
        1. Check session cache (FIX #A1 Phase 1)
        2. Check database (Azure Blob)
        3. Generate via PPI workflow (with Phase 1+2+3 optimizations)
        4. Enrich with standards
        5. Store to database
        6. Return to user

        Args:
            product_type: Product type to get schema for
            session_id: Session identifier for logging
            force_regenerate: If True, skip cache and regenerate

        Returns:
            Dictionary with schema and metadata
        """

        start_time = time.time()
        session_id = session_id or f"workflow-{int(time.time())}"

        logger.info(f"[SCHEMA_WORKFLOW] Starting schema workflow for: {product_type}")
        print(f"\n{'='*70}")
        print(f"ğŸ“‹ [SCHEMA_WORKFLOW] GET_OR_GENERATE_SCHEMA")
        print(f"   Product: {product_type}")
        print(f"   Session: {session_id}")
        print(f"{'='*70}\n")

        try:
            # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
            # â•‘  STEP 1: Check session cache (FIX #A1 from Phase 1)             â•‘
            # â•‘  Prevents redundant work within same session                     â•‘
            # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            if not force_regenerate:
                cache_key = f"{session_id}_{product_type}".lower()
                if cache_key in self.session_cache:
                    cached = self.session_cache[cache_key]
                    elapsed = time.time() - start_time
                    logger.info(
                        f"[SCHEMA_WORKFLOW] âœ“ Session cache hit (took {elapsed:.2f}s)"
                    )
                    print(f"âœ“ [SCHEMA_WORKFLOW] Using session cache (saved {100+:.0f}+ seconds!)\n")
                    return cached

            # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
            # â•‘  STEP 2: Check database (Azure Blob Storage)                    â•‘
            # â•‘  Load existing schema if available (< 1 second)                 â•‘
            # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            logger.info("[SCHEMA_WORKFLOW] Checking database for existing schema...")
            db_schema = await self._load_schema_from_database(product_type)

            if db_schema and not force_regenerate:
                elapsed = time.time() - start_time
                logger.info(f"[SCHEMA_WORKFLOW] âœ“ Loaded from database (took {elapsed:.2f}s)")
                print(f"âœ“ [SCHEMA_WORKFLOW] Found in database (< 1 second)\n")

                # Cache in session
                cache_key = f"{session_id}_{product_type}".lower()
                self.session_cache[cache_key] = db_schema

                return db_schema

            logger.info("[SCHEMA_WORKFLOW] Not in database, generating new schema...")

            # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
            # â•‘  STEP 3: Generate schema via PPI workflow                       â•‘
            # â•‘  With Phase 1+2+3 optimizations (100-120 seconds)               â•‘
            # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            print(f"ğŸ“‹ [SCHEMA_WORKFLOW] Generating new schema via PPI workflow...\n")

            if self.use_phase3_async:
                schema = await self._generate_schema_phase3_async(product_type)
            else:
                schema = await self._generate_schema_phase2_parallel(product_type)

            if not schema.get('success'):
                logger.warning(f"[SCHEMA_WORKFLOW] Schema generation failed")
                return {
                    "success": False,
                    "error": schema.get('error', 'Unknown error'),
                    "product_type": product_type
                }

            logger.info("[SCHEMA_WORKFLOW] âœ“ Schema generated")

            # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
            # â•‘  STEP 4: Enrich with standards & specifications                 â•‘
            # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            print(f"ğŸ“‹ [SCHEMA_WORKFLOW] Enriching schema with standards...\n")

            enriched_schema = await self._enrich_schema(product_type, schema)

            logger.info("[SCHEMA_WORKFLOW] âœ“ Schema enriched with standards")

            # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
            # â•‘  STEP 5: Store to database                                      â•‘
            # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            logger.info("[SCHEMA_WORKFLOW] Storing schema to database...")

            await self._store_schema_to_database(product_type, enriched_schema)

            logger.info("[SCHEMA_WORKFLOW] âœ“ Stored to database")

            # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
            # â•‘  STEP 6: Cache in session & return                              â•‘
            # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            cache_key = f"{session_id}_{product_type}".lower()
            self.session_cache[cache_key] = enriched_schema

            elapsed = time.time() - start_time

            logger.info(f"[SCHEMA_WORKFLOW] â•”{'='*68}â•—")
            logger.info(f"[SCHEMA_WORKFLOW] â•‘ SCHEMA WORKFLOW COMPLETE")
            logger.info(f"[SCHEMA_WORKFLOW] â•‘ Product: {product_type}")
            logger.info(f"[SCHEMA_WORKFLOW] â•‘ Total time: {elapsed:.2f}s")
            logger.info(f"[SCHEMA_WORKFLOW] â•‘ Optimization: {'Phase 3 Async' if self.use_phase3_async else 'Phase 2 Parallel'}")
            logger.info(f"[SCHEMA_WORKFLOW] â•š{'='*68}â•")

            print(f"âœ“ [SCHEMA_WORKFLOW] Complete in {elapsed:.2f}s\n")

            return {
                "success": True,
                "schema": enriched_schema,
                "product_type": product_type,
                "time_seconds": elapsed,
                "source": "generated",
                "optimization": "phase3_async" if self.use_phase3_async else "phase2_parallel"
            }

        except Exception as e:
            logger.error(f"[SCHEMA_WORKFLOW] Error: {e}", exc_info=True)
            elapsed = time.time() - start_time
            print(f"âœ— [SCHEMA_WORKFLOW] Error after {elapsed:.2f}s: {str(e)}\n")

            return {
                "success": False,
                "error": str(e),
                "product_type": product_type,
                "time_seconds": elapsed
            }

    async def get_or_generate_schemas_batch(
        self,
        product_types: List[str],
        session_id: Optional[str] = None
    ) -> Dict[str, Dict[str, Any]]:
        """
        Get or generate schemas for multiple products concurrently.

        Uses async/await for true concurrent generation (Phase 3).
        Multiple products generated simultaneously while benefiting from
        async non-blocking I/O.

        Args:
            product_types: List of product types
            session_id: Session identifier

        Returns:
            Dictionary mapping product_type -> schema result
        """

        logger.info(f"[SCHEMA_WORKFLOW] Batch workflow for {len(product_types)} products")
        print(f"\n{'='*70}")
        print(f"ğŸ“‹ [SCHEMA_WORKFLOW] BATCH GET_OR_GENERATE_SCHEMAS")
        print(f"   Products: {len(product_types)}")
        print(f"{'='*70}\n")

        # Create concurrent tasks for all products
        tasks = [
            self.get_or_generate_schema(product_type, session_id)
            for product_type in product_types
        ]

        # Execute concurrently
        results_list = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results
        results = {}
        for product_type, result in zip(product_types, results_list):
            if isinstance(result, Exception):
                logger.error(f"[SCHEMA_WORKFLOW] Batch error for {product_type}: {result}")
                results[product_type] = {
                    "success": False,
                    "error": str(result)
                }
            else:
                results[product_type] = result

        successful = sum(1 for r in results.values() if r.get('success'))
        logger.info(f"[SCHEMA_WORKFLOW] Batch complete: {successful}/{len(product_types)} successful")

        print(f"âœ“ [SCHEMA_WORKFLOW] Batch complete: {successful}/{len(product_types)} successful\n")

        return results

    async def _generate_schema_phase3_async(self, product_type: str) -> Dict[str, Any]:
        """
        Generate schema using Phase 3 async optimizations.

        Uses async/await for non-blocking I/O during PPI workflow.

        Args:
            product_type: Product type

        Returns:
            Schema result dictionary
        """

        try:
            from agentic.deep_agent.schema.generation.async_generator import generate_schemas_async

            logger.info(f"[SCHEMA_WORKFLOW] Using Phase 3 async generation")

            results = await generate_schemas_async([product_type], max_concurrent=1)

            schema_result = results.get(product_type, {})

            if schema_result.get('success'):
                return {
                    "success": True,
                    "schema": schema_result.get('schema'),
                    "source": "ppi_workflow_async"
                }
            else:
                return {
                    "success": False,
                    "error": schema_result.get('error', 'Generation failed')
                }

        except ImportError:
            logger.warning("[SCHEMA_WORKFLOW] Phase 3 async not available, falling back to Phase 2")
            return await self._generate_schema_phase2_parallel(product_type)

    async def _generate_schema_phase2_parallel(self, product_type: str) -> Dict[str, Any]:
        """
        Generate schema using Phase 2 parallel optimizations (ThreadPoolExecutor).

        Fallback when Phase 3 async not available.

        Args:
            product_type: Product type

        Returns:
            Schema result dictionary
        """

        try:
            from agentic.deep_agent.schema.generation.parallel_generator import generate_schemas_parallel

            logger.info(f"[SCHEMA_WORKFLOW] Using Phase 2 parallel generation")

            # Run blocking operation in thread pool
            loop = asyncio.get_event_loop()
            results = await loop.run_in_executor(
                None,
                lambda: generate_schemas_parallel([product_type], max_workers=1)
            )

            schema_result = results.get(product_type, {})

            if schema_result.get('success'):
                return {
                    "success": True,
                    "schema": schema_result.get('schema'),
                    "source": "ppi_workflow_parallel"
                }
            else:
                return {
                    "success": False,
                    "error": schema_result.get('error', 'Generation failed')
                }

        except ImportError:
            logger.warning("[SCHEMA_WORKFLOW] Phase 2 parallel not available, using sequential")
            return await self._generate_schema_sequential(product_type)

    async def _generate_schema_sequential(self, product_type: str) -> Dict[str, Any]:
        """
        Generate schema sequentially (fallback).

        This is the baseline, no optimizations.

        Args:
            product_type: Product type

        Returns:
            Schema result dictionary
        """

        try:
            logger.warning("[SCHEMA_WORKFLOW] Using sequential schema generation (no optimizations)")

            loop = asyncio.get_event_loop()

            def run_ppi():
                """Run PPI workflow."""
                try:
                    from product_search_workflow.ppi_workflow import compile_ppi_workflow

                    workflow = compile_ppi_workflow()
                    result = workflow.invoke({"product_type": product_type})
                    return result.get("schema", {})

                except Exception as e:
                    logger.error(f"[SCHEMA_WORKFLOW] PPI workflow error: {e}")
                    raise

            schema = await loop.run_in_executor(None, run_ppi)

            if schema:
                return {"success": True, "schema": schema, "source": "ppi_workflow_sequential"}
            else:
                return {"success": False, "error": "Empty schema from PPI"}

        except Exception as e:
            logger.error(f"[SCHEMA_WORKFLOW] Sequential generation failed: {e}")
            return {"success": False, "error": str(e)}

    async def _enrich_schema(
        self,
        product_type: str,
        schema: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Enrich schema with standards and specifications.

        Uses Phase 3 async or Phase 2 parallel enrichment.

        Args:
            product_type: Product type
            schema: Schema to enrich

        Returns:
            Enriched schema
        """

        try:
            if self.use_phase3_async:
                from agentic.workflows.standards_rag.async_standards_enrichment import enrich_schema_async

                enriched = await enrich_schema_async(product_type, schema)
            else:
                from agentic.workflows.standards_rag.parallel_standards_enrichment import enrich_schema_parallel

                loop = asyncio.get_event_loop()
                enriched = await loop.run_in_executor(
                    None,
                    lambda: enrich_schema_parallel(product_type, schema)
                )

            return enriched

        except Exception as e:
            logger.warning(f"[SCHEMA_WORKFLOW] Enrichment failed, returning base schema: {e}")
            return schema

    async def _load_schema_from_database(self, product_type: str) -> Optional[Dict[str, Any]]:
        """Load schema from database (Azure Blob)."""

        try:
            loop = asyncio.get_event_loop()

            def load_from_db():
                """Load from Azure (blocking)."""
                try:
                    from tools.schema_tools import load_schema_from_azure

                    return load_schema_from_azure(product_type)

                except Exception as e:
                    logger.debug(f"[SCHEMA_WORKFLOW] Not in database: {e}")
                    return None

            schema = await loop.run_in_executor(None, load_from_db)
            return schema

        except Exception as e:
            logger.warning(f"[SCHEMA_WORKFLOW] Database lookup failed: {e}")
            return None

    async def _store_schema_to_database(
        self,
        product_type: str,
        schema: Dict[str, Any]
    ) -> bool:
        """Store schema to database (Azure Blob)."""

        try:
            loop = asyncio.get_event_loop()

            def save_to_db():
                """Save to Azure (blocking)."""
                try:
                    from tools.schema_tools import save_schema_to_azure

                    save_schema_to_azure(product_type, schema)
                    return True

                except Exception as e:
                    logger.warning(f"[SCHEMA_WORKFLOW] Failed to save to database: {e}")
                    return False

            result = await loop.run_in_executor(None, save_to_db)
            return result

        except Exception as e:
            logger.warning(f"[SCHEMA_WORKFLOW] Database storage failed: {e}")
            return False

    def clear_session_cache(self):
        """Clear session-level cache (call at end of session)."""

        cache_size = len(self.session_cache)
        self.session_cache.clear()
        logger.info(f"[SCHEMA_WORKFLOW] Cleared session cache ({cache_size} entries)")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HELPER FUNCTION FOR INTEGRATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


async def get_or_generate_schema(
    product_type: str,
    session_id: Optional[str] = None,
    force_regenerate: bool = False,
    use_phase3: bool = True
) -> Dict[str, Any]:
    """
    Convenience function to get or generate schema.

    Complete workflow:
    1. Check session cache
    2. Check database
    3. Generate if needed (with Phase 1+2+3 optimizations)
    4. Enrich with standards
    5. Store to database
    6. Return result

    Example:
        >>> schema = await get_or_generate_schema("Temperature Transmitter")
        >>> # Automatic workflow handling

    Args:
        product_type: Product type
        session_id: Session ID for tracking
        force_regenerate: Force regeneration
        use_phase3: Use Phase 3 async (default True)

    Returns:
        Schema result dictionary
    """

    workflow = SchemaWorkflow(use_phase3_async=use_phase3)
    return await workflow.get_or_generate_schema(
        product_type,
        session_id=session_id,
        force_regenerate=force_regenerate
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TESTING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def test_schema_workflow():
    """Test complete schema workflow."""

    print("\n" + "="*70)
    print("SCHEMA WORKFLOW - COMPLETE TEST")
    print("="*70)

    workflow = SchemaWorkflow(use_phase3_async=True)

    # Test 1: Single product
    print("\n[Test 1] Single product:")
    result = await workflow.get_or_generate_schema(
        "Temperature Transmitter",
        session_id="test_001"
    )
    print(f"  Success: {result.get('success')}")
    print(f"  Time: {result.get('time_seconds'):.2f}s")
    print(f"  Source: {result.get('source')}")

    # Test 2: Batch generation
    print("\n[Test 2] Batch generation (3 products):")
    results = await workflow.get_or_generate_schemas_batch(
        ["Temperature Transmitter", "Pressure Gauge", "Level Switch"],
        session_id="test_001"
    )
    successful = sum(1 for r in results.values() if r.get('success'))
    print(f"  Successful: {successful}/{len(results)}")

    print("\n" + "="*70)


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    asyncio.run(test_schema_workflow())
